/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
(function webpackUniversalModuleDefinition(root, factory) {
	//CommonJS2 Comment
	if(typeof exports === 'object' && typeof module === 'object')
		module.exports = factory();
	//AMD Comment
	else if(typeof define === 'function' && define.amd)
		define([], factory);
	//CommonJS Comment
	else if(typeof exports === 'object')
		exports["sema-engine"] = factory();
	//Root Comment
	else
		root["sema-engine"] = factory();
})(self, function() {
return /******/ (() => { // webpackBootstrap
/******/ 	"use strict";
/******/ 	var __webpack_modules__ = ({

/***/ "./src/engine/ringbuf.js":
/*!*******************************!*\
  !*** ./src/engine/ringbuf.js ***!
  \*******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => __WEBPACK_DEFAULT_EXPORT__\n/* harmony export */ });\n/* harmony default export */ const __WEBPACK_DEFAULT_EXPORT__ = (__webpack_require__.p + \"ringbuf.js\");\n\n//# sourceURL=webpack://sema-engine/./src/engine/ringbuf.js?");

/***/ }),

/***/ "./src/engine/audioEngine.js":
/*!***********************************!*\
  !*** ./src/engine/audioEngine.js ***!
  \***********************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"AudioEngine\": () => /* binding */ AudioEngine\n/* harmony export */ });\n/* harmony import */ var _ringbuf_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./ringbuf.js */ \"./src/engine/ringbuf.js\");\n/* harmony import */ var _maximilian_util_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./maximilian.util.js */ \"./src/engine/maximilian.util.js\");\n// import Module from './maximilian.wasmmodule.js'; //NOTE:FB We need this import here for webpack to emit maximilian.wasmmodule.js\n// import Open303 from './open303.wasmmodule.js'; //NOTE:FB We need this import here for webpack to emit maximilian.wasmmodule.js\n// import CustomProcessor from './maxi-processor.js';\n //thanks padenot\n\n// import {\n//   kuramotoNetClock\n// } from './interfaces/clockInterface.js';\n// import {\n//   PubSub\n// } from './messaging/pubSub.js';\n// import {\n//   PeerStreaming\n// } from '../interfaces/peerStreaming.js';\n// import {\n//   copyToPasteBuffer\n// } from '../utils/pasteBuffer.js';\n\n\n/**\n * The CustomMaxiNode is a class that extends AudioWorkletNode\n * to hold an Custom Audio Worklet Processor and connect to Web Audio graph\n * @class CustomMaxiNode\n * @extends AudioWorkletNode\n */\n// if(true){\nclass CustomMaxiNode extends AudioWorkletNode {\n  constructor(audioContext, processorName) {\n    // super(audioContext, processorName);\n    console.log();\n    let options = {\n      numberOfInputs: 1,\n      numberOfOutputs: 1,\n      outputChannelCount: [audioContext.destination.maxChannelCount]\n    };\n    super(audioContext, processorName, options);\n  }\n}\n// }\n\n/**\n * The AudioEngine is a singleton class that encapsulates the AudioContext\n * and all WASM and Maximilian -powered Audio Worklet Processor\n * @class AudioEngine\n */\nclass AudioEngine {\n\t/**\n\t * @constructor\n\t */\n\tconstructor() {\n\t\tif (AudioEngine.instance) {\n\t\t\treturn AudioEngine.instance; // Singleton pattern\n\t\t}\n\t\tAudioEngine.instance = this;\n\n\t\t// Hash of on-demand analysers (e.g. spectrogram, oscilloscope)\n\t\t// NOTE: analysers from localStorage are loaded from local Storage before user-started audioContext init\n\t\tthis.analysers = {};\n\n\t\t//shared array buffers for sharing client side data to the audio engine- e.g. mouse coords\n\t\tthis.sharedArrayBuffers = {};\n\n\t\t// MOVE THIS TO AN UP LAYER IN SEMA\n\n\t\t// Sema's Publish-Subscribe pattern object with 'lowercase-lowercase' format convention for subscription topic\n\t\t// this.messaging = new PubSub();\n\t\t// this.messaging.subscribe('eval-dsp', e => this.evalDSP(e));\n\t\t// this.messaging.subscribe('stop-audio', e => this.stop());\n\t\t// this.messaging.subscribe('load-sample', (name, url) =>\n\t\t//   this.loadSample(name, url)\n\t\t// );\n\t\t// this.messaging.subscribe('model-output-data', e =>\n\t\t//   this.onMessagingEventHandler(e)\n\t\t// );\n\t\t// this.messaging.subscribe('clock-phase', e =>\n\t\t//   this.onMessagingEventHandler(e)\n\t\t// );\n\t\t// this.messaging.subscribe('model-send-buffer', e =>\n\t\t//   this.onMessagingEventHandler(e)\n\t\t// );\n\t\t// this.messaging.subscribe('add-engine-analyser', e =>\n\t\t//   this.createAnalyser(e)\n\t\t// );\n\t\t// this.messaging.subscribe('remove-engine-analyser', e =>\n\t\t//   this.removeAnalyser(e)\n\t\t// );\n\n\t\t// this.messaging.subscribe('mouse-xy', e => {\n\t\t//   if (this.sharedArrayBuffers.mxy) {\n\t\t//     this.sharedArrayBuffers.mxy.rb.push(e);\n\t\t//   }\n\t\t// });\n\t\t// this.messaging.subscribe('osc', e => console.log(`DEBUG:AudioEngine:OSC: ${e}`));\n\n\t\t//temporarily disabled for now\n\t\t// this.kuraClock = new kuramotoNetClock();\n\n\t\t//temporarily disabled for now\n\t\t// this.peerNet = new PeerStreaming();\n\n\t\t//the message has incoming data from other peers\n\t\t// this.messaging.subscribe('peermsg', (e) => {\n\t\t//   e.ttype = 'NET';\n\t\t//   e.peermsg = 1;\n\t\t//   this.onMessagingEventHandler(e);\n\t\t// });\n\n\t\t// this.messaging.subscribe('peerinfo-request', (e) => {\n\t\t//   console.log(this.peerNet.peerID);\n\t\t//   copyToPasteBuffer(this.peerNet.peerID);\n\t\t// });\n\t}\n\n\t/**\n\t * Handler of audio worklet processor events\n\t * @onProcessorMessageEventHandler\n\t */\n\tonProcessorMessageEventHandler(event) {\n\t\tif (event != undefined && event.data != undefined) {\n\t\t\t// console.log('DEBUG:AudioEngine:processorMessageHandler:');\n\t\t\t// console.log(event);\n\t\t\tif (event.data.rq != undefined && event.data.rq === \"send\") {\n\t\t\t\tswitch (event.data.ttype) {\n\t\t\t\t\tcase \"ML\":\n\t\t\t\t\t\t// Stream generated by 'toJS' live code instruction — e.g. {10,0,{1}sin}toJS;\n\t\t\t\t\t\t// publishes to model/JS editor, which posts to ml.worker\n\t\t\t\t\t\tthis.messaging.publish(\"model-input-data\", {\n\t\t\t\t\t\t\ttype: \"model-input-data\",\n\t\t\t\t\t\t\tvalue: event.data.value,\n\t\t\t\t\t\t\tch: event.data.ch,\n\t\t\t\t\t\t});\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase \"NET\":\n\t\t\t\t\t\tthis.peerNet.send(\n\t\t\t\t\t\t\tevent.data.ch[0],\n\t\t\t\t\t\t\tevent.data.value,\n\t\t\t\t\t\t\tevent.data.ch[1]\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (event.data.rq && event.data.rq === \"buf\") {\n\t\t\t\tconsole.log(\"buf\", event.data);\n\t\t\t\tswitch (event.data.ttype) {\n\t\t\t\t\tcase \"ML\":\n\t\t\t\t\t\tthis.messaging.publish(\"model-input-buffer\", {\n\t\t\t\t\t\t\ttype: \"model-input-buffer\",\n\t\t\t\t\t\t\tvalue: event.data.value,\n\t\t\t\t\t\t\tchannelID: event.data.channelID, //channel ID\n\t\t\t\t\t\t\tblocksize: event.data.blocksize,\n\t\t\t\t\t\t});\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (event.data === \"giveMeSomeSamples\") {\n\t\t\t} else if (event.data.phase != undefined) {\n\t\t\t\t// console.log('DEBUG:AudioEngine:phase:');\n\t\t\t\t// console.log(event.data.phase);\n\t\t\t\t// this.kuraClock.broadcastPhase(event.data.phase); // TODO Refactor p to phase\n\t\t\t}\n\t\t\t// else if (event.data.rq != undefined && event.data.rq === 'receive') {\n\t\t\t//   switch (event.data.ttype) {\n\t\t\t//     case 'ML':\n\t\t\t//       // Stream generated by 'fromJS' live code instruction – e.g. {{10,1}fromJS}saw\n\t\t\t//       // publishes to model/JS editor, which posts to ml.worker\n\t\t\t//       this.messaging.publish('model-output-data-request', {\n\t\t\t//         type: 'model-output-data-request',\n\t\t\t//         value: event.data.value,\n\t\t\t//         channel: event.data.ch\n\t\t\t//       });\n\t\t\t//       break;\n\t\t\t//     case 'NET':\n\t\t\t//       break;\n\t\t\t//   }\n\t\t\t// }\n\t\t}\n\t}\n\n\t/**\n\t * Handler of the Pub/Sub message events\n\t * whose topics are subscribed to in the audio engine constructor\n\t * @onMessagingEventHandler\n\t */\n\tonMessagingEventHandler(event) {\n\t\tif (event !== undefined) {\n\t\t\t// Receive notification from 'model-output-data' topic\n\t\t\tconsole.log(\"DEBUG:AudioEngine:onMessagingEventHandler:\");\n\t\t\tconsole.log(event);\n\t\t\tthis.audioWorkletNode.port.postMessage(event);\n\t\t}\n\t}\n\n\t/**\n\t * Creates a WAAPI analyser node\n\t * @todo configuration object as argumen\n\t * @createAnalyser\n\t */\n\tcreateAnalyser(event) {\n\t\t// If Analyser creation happens after AudioContext intialization, create and connect WAAPI analyser\n\t\tif (this.audioContext !== undefined && event !== undefined) {\n\t\t\tlet analyser = this.audioContext.createAnalyser();\n\t\t\tanalyser.smoothingTimeConstant = 0.25;\n\t\t\tanalyser.fftSize = 256; // default 2048;\n\t\t\tanalyser.minDecibels = -90; // default\n\t\t\tanalyser.maxDecibels = -0; // default -10; max 0\n\t\t\tthis.connectAnalyser(analyser, event.id); // @todo Move out\n\n\t\t\t// Other if AudioContext is NOT created yet (after app load, before splashScreen click)\n\t\t} else if (this.audioContext === undefined) {\n\t\t\tthis.analysers[event.id] = {};\n\t\t}\n\t}\n\n\t/**\n\t * Polls data from connected WAAPI analyser return structured object with data and time data in arrays\n\t * @param {*} analyser\n\t */\n\n\tpollAnalyserData(analyser) {\n\t\tif (analyser !== undefined) {\n\t\t\tconst timeDataArray = new Uint8Array(analyser.fftSize); // Uint8Array should be the same length as the fftSize\n\t\t\tconst frequencyDataArray = new Uint8Array(analyser.fftSize);\n\n\t\t\tanalyser.getByteTimeDomainData(timeDataArray);\n\t\t\tanalyser.getByteFrequencyData(frequencyDataArray);\n\n\t\t\treturn {\n\t\t\t\tsmoothingTimeConstant: analyser.smoothingTimeConstant,\n\t\t\t\tfftSize: analyser.fftSize,\n\t\t\t\tfrequencyDataArray: frequencyDataArray,\n\t\t\t\ttimeDataArray: timeDataArray,\n\t\t\t};\n\t\t}\n\t}\n\n\t/**\n\t * Connects WAAPI analyser node to the main audio worklet for visualisation.\n\t * @connectAnalyser\n\t */\n\tconnectAnalyser(analyser, name) {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tthis.audioWorkletNode.connect(analyser);\n\n\t\t\tlet analyserFrameId;\n\t\t\tlet analyserData;\n\n\t\t\t/**\n\t\t\t * Creates requestAnimationFrame loop for polling data and publishing\n\t\t\t * Returns Analyser Frame ID for adding to Analysers hash and cancelling animation frame\n\t\t\t */\n\t\t\tconst analyserPollingLoop = () => {\n\t\t\t\tanalyserData = this.pollAnalyserData(analyser);\n\t\t\t\tthis.messaging.publish(\"analyser-data\", analyserData);\n\t\t\t\tlet analyserFrameId = requestAnimationFrame(analyserPollingLoop);\n\t\t\t\tthis.analysers[name] = {\n\t\t\t\t\tanalyser,\n\t\t\t\t\tanalyserFrameId,\n\t\t\t\t};\n\t\t\t\treturn analyserFrameId;\n\t\t\t};\n\n\t\t\t// analyserFrameId = analyserPollingLoop;\n\n\t\t\tanalyserPollingLoop();\n\t\t}\n\t}\n\n\tconnectAnalysers() {\n\t\tObject.keys(this.analysers).map((id) =>\n\t\t\tthis.createAnalyser({\n\t\t\t\tid,\n\t\t\t})\n\t\t);\n\t}\n\n\t/**\n\t * Removes a WAAPI analyser node, disconnects graph, cancels animation frame, deletes from hash\n\t * @removeAnalyser\n\t */\n\tremoveAnalyser(event) {\n\t\tif (\n\t\t\tthis.audioContext !== undefined &&\n\t\t\tthis.audioWorkletNode !== undefined\n\t\t) {\n\t\t\tlet analyser = this.analysers[event.id];\n\t\t\tif (analyser !== undefined) {\n\t\t\t\tcancelAnimationFrame(this.analysers[event.id].analyserFrameId);\n\t\t\t\tdelete this.analysers[event.id];\n\t\t\t\t// this.audioWorkletNode.disconnect(analyser);\n\t\t\t}\n\t\t}\n\t}\n\n\t//make a shared array buffer for communicating with the audio engine\n\tcreateSharedArrayBuffer(chID, ttype, blocksize) {\n\t\tlet sab = _ringbuf_js__WEBPACK_IMPORTED_MODULE_0__.default.getStorageForCapacity(32 * blocksize, Float64Array);\n\t\tlet ringbuf = new _ringbuf_js__WEBPACK_IMPORTED_MODULE_0__.default(sab, Float64Array);\n\n\t\tthis.audioWorkletNode.port.postMessage({\n\t\t\tfunc: \"sab\",\n\t\t\tvalue: sab,\n\t\t\tttype: ttype,\n\t\t\tchannelID: chID,\n\t\t\tblocksize: blocksize,\n\t\t});\n\n\t\tthis.sharedArrayBuffers[chID] = {\n\t\t\tsab: sab,\n\t\t\trb: ringbuf,\n\t\t};\n\n\t\tconsole.log(this.sharedArrayBuffers);\n\t}\n\n\t/**\n\t * Initialises audio context and sets worklet processor code\n\t * @play\n\t */\n\tasync init(audioWorkletURL /*numClockPeers*/) {\n\t\t// AudioContext needs lazy loading to workaround the Chrome warning\n\t\t// Audio Engine first play() call, triggered by user, prevents the warning\n\t\t// by setting this.audioContext = new AudioContext();\n\t\tthis.audioContext;\n\t\tthis.audioWorkletProcessorName = \"maxi-processor\";\n\t\tthis.audioWorkletUrl = audioWorkletURL;\n\t\tthis.samplesLoaded = false;\n\n\t\tif (this.audioContext === undefined) {\n\t\t\tthis.audioContext = new AudioContext({\n\t\t\t\t// create audio context with latency optimally configured for playback\n\t\t\t\tlatencyHint: \"playback\",\n\t\t\t\t// latencyHint: 32/44100,  //this doesn't work below 512 on chrome (?)\n\t\t\t\t// sampleRate: 44100\n\t\t\t});\n\n\t\t\tthis.audioContext.destination.channelInterpretation = \"discrete\";\n\t\t\tthis.audioContext.destination.channelCountMode = \"explicit\";\n\t\t\tthis.audioContext.destination.channelCount = this.audioContext.destination.maxChannelCount;\n\t\t\t// console.log(this.audioContext.destination);\n\n\t\t\tawait this.loadWorkletProcessorCode();\n\n\t\t\t// Connect the worklet node to the audio graph\n\t\t\tthis.audioWorkletNode.connect(this.audioContext.destination);\n\n\t\t\t// this.audioWorkletNode.channelInterpretation = 'discrete';\n\t\t\t// this.audioWorkletNode.channelCountMode = 'explicit';\n\t\t\t// this.audioWorkletNode.channelCount = this.audioContext.destination.maxChannelCount;\n\n\t\t\t// this.connectMediaStream();\n\n\t\t\t// this.connectAnalysers(); // Connect Analysers loaded from the store\n\n\t\t\t// this.loadImportedSamples();\n\n\t\t\t// No need to inject the callback here, messaging is built in KuraClock\n\t\t\t// this.kuraClock = new kuramotoNetClock((phase, idx) => {\n\t\t\t//   // console.log( `DEBUG:AudioEngine:sendPeersMyClockPhase:phase:${phase}:id:${idx}`);\n\t\t\t//   // This requires an initialised audio worklet\n\t\t\t//   this.audioWorkletNode.port.postMessage({ phase: phase, i: idx });\n\t\t\t// });\n\n\t\t\t//temporarily disabled\n\t\t\t// if (this.kuraClock.connected()) {\n\t\t\t// \tthis.kuraClock.queryPeers(async numClockPeers => {\n\t\t\t// \t\tconsole.log(`DEBUG:AudioEngine:init:numClockPeers: ${numClockPeers}`);\n\t\t\t// \t});\n\t\t\t// }\n\n\t\t\tthis.createSharedArrayBuffer(\"mxy\", \"mouseXY\", 2);\n\t\t}\n\t}\n\n\t/**\n\t * Initialises audio context and sets worklet processor code\n\t * or re-starts audio playback by stopping and running the latest Audio Worklet Processor code\n\t * @play\n\t */\n\tplay() {\n\t\tif (this.audioContext !== undefined) {\n\t\t\tif (this.audioContext.state !== \"suspended\") {\n\t\t\t\tthis.stop();\n\t\t\t\treturn false;\n\t\t\t} else {\n\t\t\t\tthis.audioContext.resume();\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Suspends AudioContext (Pause)\n\t * @stop\n\t */\n\tstop() {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tthis.audioContext.suspend();\n\t\t}\n\t}\n\n\t/**\n\t * Stops audio by disconnecting AudioNode with AudioWorkletProcessor code\n\t * from Web Audio graph TODO Investigate when it is best to just STOP the graph exectution\n\t * @stop\n\t */\n\tstopAndRelease() {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tthis.audioWorkletNode.disconnect(this.audioContext.destination);\n\t\t\tthis.audioWorkletNode = undefined;\n\t\t}\n\t}\n\n\tmore(gain) {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tconst gainParam = this.audioWorkletNode.parameters.get(gain);\n\t\t\tgainParam.value += 0.5;\n\t\t\tconsole.log(gain + \": \" + gainParam.value); // DEBUG\n\t\t\treturn true;\n\t\t} else return false;\n\t}\n\n\tless(gain) {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tconst gainParam = this.audioWorkletNode.parameters.get(gain);\n\t\t\tgainParam.value -= 0.5;\n\t\t\tconsole.log(gain + \": \" + gainParam.value); // DEBUG\n\t\t\treturn true;\n\t\t} else return false;\n\t}\n\n\teval(dspFunction) {\n\t\t// console.log('DEBUG:AudioEngine:evalDSP:');\n\t\t// console.log(dspFunction);\n\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tif (this.audioContext.state === \"suspended\") {\n\t\t\t\tthis.audioContext.resume();\n\t\t\t}\n\t\t\tthis.audioWorkletNode.port.postMessage({\n\t\t\t\teval: 1,\n\t\t\t\tsetup: dspFunction.setup,\n\t\t\t\tloop: dspFunction.loop,\n\t\t\t});\n\t\t\treturn true;\n\t\t} else return false;\n\t}\n\n\tsendClockPhase(phase, idx) {\n\t\tif (this.audioWorkletNode !== undefined) {\n\t\t\tthis.audioWorkletNode.port.postMessage({\n\t\t\t\tphase: phase,\n\t\t\t\ti: idx,\n\t\t\t});\n\t\t}\n\t}\n\n\tonAudioInputInit(stream) {\n\t\t// console.log('DEBUG:AudioEngine: Audio Input init');\n\t\tlet mediaStreamSource = this.audioContext.createMediaStreamSource(stream);\n\t\tmediaStreamSource.connect(this.audioWorkletNode);\n\t}\n\n\tonAudioInputFail(error) {\n\t\tconsole.log(\n\t\t\t`DEBUG:AudioEngine:AudioInputFail: ${error.message} ${error.name}`\n\t\t);\n\t}\n\n\t/**\n\t * Sets up an AudioIn WAAPI sub-graph\n\t * @connectMediaStreamSourceInput\n\t */\n\tasync connectMediaStream() {\n\t\tconst constraints = (window.constraints = {\n\t\t\taudio: true,\n\t\t\tvideo: false,\n\t\t});\n\n\t\tnavigator.mediaDevices\n\t\t\t.getUserMedia(constraints)\n\t\t\t.then((s) => this.onAudioInputInit(s))\n\t\t\t.catch(this.onAudioInputFail);\n\t}\n\n\t/**\n\t * Loads audioWorklet processor code into a worklet,\n\t * setups up all handlers (errors, async messaging, etc),\n\t * connects the worklet processor to the WAAPI graph\n\t */\n\tasync loadWorkletProcessorCode() {\n\t\tif (this.audioContext !== undefined) {\n\t\t\ttry {\n\t\t\t\tawait this.audioContext.audioWorklet.addModule(this.audioWorkletUrl);\n\t\t\t} catch (err) {\n\t\t\t\tconsole.error(\n\t\t\t\t\t\"ERROR: AudioEngine:loadWorkletProcessorCode: AudioWorklet not supported in this browser: \",\n\t\t\t\t\terr.message\n\t\t\t\t);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\ttry {\n\t\t\t\t// Custom node constructor with required parameters\n\t\t\t\tthis.audioWorkletNode = new CustomMaxiNode(\n\t\t\t\t\tthis.audioContext,\n\t\t\t\t\tthis.audioWorkletProcessorName\n\t\t\t\t);\n\n\t\t\t\t// All possible error event handlers subscribed\n\t\t\t\tthis.audioWorkletNode.onprocessorerror = (event) => {\n\t\t\t\t\t// Errors from the processor\n\t\t\t\t\tconsole.log(\n\t\t\t\t\t\t`DEBUG:AudioEngine:loadWorkletProcessorCode: MaxiProcessor Error detected`\n\t\t\t\t\t);\n\t\t\t\t};\n\n\t\t\t\tthis.audioWorkletNode.port.onmessageerror = (event) => {\n\t\t\t\t\t//  error from the processor port\n\t\t\t\t\tconsole.log(\n\t\t\t\t\t\t`DEBUG:AudioEngine:loadWorkletProcessorCode: Error message from port: ` +\n\t\t\t\t\t\t\tevent.data\n\t\t\t\t\t);\n\t\t\t\t};\n\n\t\t\t\t// State changes in the audio worklet processor\n\t\t\t\tthis.audioWorkletNode.onprocessorstatechange = (event) => {\n\t\t\t\t\tconsole.log(\n\t\t\t\t\t\t`DEBUG:AudioEngine:loadWorkletProcessorCode: MaxiProcessor state change detected: ` +\n\t\t\t\t\t\t\taudioWorkletNode.processorState\n\t\t\t\t\t);\n\t\t\t\t};\n\n\t\t\t\t// Worklet Processor message handler\n\t\t\t\tthis.audioWorkletNode.port.onmessage = (event) => {\n\t\t\t\t\tthis.onProcessorMessageEventHandler(event);\n\t\t\t\t};\n\n\t\t\t\treturn true;\n\t\t\t} catch (err) {\n\t\t\t\tconsole.error(\n\t\t\t\t\t\"ERROR: AudioEngine:loadWorkletProcessorCode: Custom AudioWorklet node creation: \",\n\t\t\t\t\terr.message\n\t\t\t\t);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tloadSample(objectName, url) {\n\t\tif (this.audioContext !== undefined) {\n\t\t\t(0,_maximilian_util_js__WEBPACK_IMPORTED_MODULE_1__.loadSampleToArray)(\n\t\t\t\tthis.audioContext,\n\t\t\t\tobjectName,\n\t\t\t\turl,\n\t\t\t\tthis.audioWorkletNode\n\t\t\t);\n\t\t} else throw \"Audio Context is not initialised!\";\n\t}\n\n\t// getSamplesNamegetSamplesNamess() {\n\t// \tconst r = require.context(\"../../assets/samples\", false, /\\.wav$/);\n\n\t// \t// return an array list of filenames (with extension)\n\t// \tconst importAll = (r) => r.keys().map((file) => file.match(/[^\\/]+$/)[0]);\n\n\t// \treturn importAll(r);\n\t// }\n\n\t// lazyLoadSample(sampleName) {\n\t// \timport(/* webpackMode: 'lazy' */ `../../assets/samples/${sampleName}`)\n\t// \t\t.then(() => this.loadSample(sampleName, `/samples/${sampleName}`))\n\t// \t\t.catch((err) =>\n\t// \t\t\tconsole.error(`DEBUG:AudioEngine:lazyLoadSample: ` + err)\n\t// \t\t);\n\t// }\n\n\n\t// loadImportedSamples() {\n\t// \tlet samplesNames = this.getSamplesNames();\n\t// \t// console.log('DEBUG:AudioEngine:getSamplesNames: ' + samplesNames);\n\t// \tsamplesNames.forEach((sampleName) => {\n\t// \t\tthis.lazyLoadSample(sampleName);\n\t// \t});\n\t// }\n\n\t// NOTE:FB Test code should be segregated from production code into its own fixture.\n\t// Otherwise, it becomes bloated, difficult to read and reason about.\n\t// messageHandler(data) {\n\t// \tif (data == 'dspStart') {\n\t// \t\tthis.ts = window.performance.now();\n\t// \t}\n\t// \tif (data == 'dspEnd') {\n\t// \t\tthis.ts = window.performance.now() - this.ts;\n\t// \t\tthis.dspTime = this.dspTime * 0.9 + this.ts * 0.1; //time for 128 sample buffer\n\t// \t\tthis.onNewDSPLoadValue((this.dspTime / 2.90249433106576) * 100);\n\t// \t}\n\t// \tif (data == 'evalEnd') {\n\t// \t\tlet evalts = window.performance.now();\n\t// \t\tthis.onEvalTimestamp(evalts);\n\t// \t} else if (data == 'evalEnd') {\n\t// \t\tlet evalts = window.performance.now();\n\t// \t\tthis.onEvalTimestamp(evalts);\n\t// \t} else if (data == 'giveMeSomeSamples') {\n\t// \t\t// this.msgHandler('giveMeSomeSamples');    \t// NOTE:FB Untangling the previous msgHandler hack from the audio engine\n\t// \t} else {\n\t// \t\tthis.msgHandler(data);\n\t// \t}\n\t// }\n}\n\n\n\n//# sourceURL=webpack://sema-engine/./src/engine/audioEngine.js?");

/***/ }),

/***/ "./src/engine/maximilian.util.js":
/*!***************************************!*\
  !*** ./src/engine/maximilian.util.js ***!
  \***************************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"getArrayAsVectorDbl\": () => /* binding */ getArrayAsVectorDbl,\n/* harmony export */   \"getBase64\": () => /* binding */ getBase64,\n/* harmony export */   \"_keyStr\": () => /* binding */ _keyStr,\n/* harmony export */   \"removePaddingFromBase64\": () => /* binding */ removePaddingFromBase64,\n/* harmony export */   \"loadSampleToArray\": () => /* binding */ loadSampleToArray,\n/* harmony export */   \"buildWorkletStringForBlob\": () => /* binding */ buildWorkletStringForBlob,\n/* harmony export */   \"createAndRegisterCustomProcessorCode\": () => /* binding */ createAndRegisterCustomProcessorCode,\n/* harmony export */   \"buildWorkletFromBlob\": () => /* binding */ buildWorkletFromBlob,\n/* harmony export */   \"runProcessorCode\": () => /* binding */ runProcessorCode,\n/* harmony export */   \"generateNoiseBuffer\": () => /* binding */ generateNoiseBuffer,\n/* harmony export */   \"translateBlobToBuffer\": () => /* binding */ translateBlobToBuffer\n/* harmony export */ });\nconst getArrayAsVectorDbl = (arrayIn) => {\n  var vecOut = new exports.VectorDouble();\n  for (var i = 0; i < arrayIn.length; i++) {\n    vecOut.push_back(arrayIn[i]);\n  }\n  return vecOut;\n};\n\nconst getBase64 = (str) => {\n  //check if the string is a data URI\n  if (str.indexOf(';base64,') !== -1) {\n    //see where the actual data begins\n    var dataStart = str.indexOf(';base64,') + 8;\n    //check if the data is base64-encoded, if yes, return it\n    // taken from\n    // http://stackoverflow.com/a/8571649\n    return str.slice(dataStart).match(/^([A-Za-z0-9+\\/]{4})*([A-Za-z0-9+\\/]{4}|[A-Za-z0-9+\\/]{3}=|[A-Za-z0-9+\\/]{2}==)$/) ? str.slice(dataStart) : false;\n  } else return false;\n};\n\nconst _keyStr = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\";\n\nconst removePaddingFromBase64 = (input) => {\n  var lkey = Module.maxiTools._keyStr.indexOf(input.charAt(input.length - 1));\n  if (lkey === 64) {\n    return input.substring(0, input.length - 1);\n  }\n  return input;\n};\n\n\nconst loadSampleToArray = (audioContext, sampleObjectName, url, audioWorkletNode) => {\n  var data = [];\n\n  var context = audioContext;\n\n  //check if url is actually a base64-encoded string\n  var b64 = getBase64(url);\n  if (b64) {\n    //convert to arraybuffer\n    //modified version of this:\n    // https://github.com/danguer/blog-examples/blob/master/js/base64-binary.js\n    var ab_bytes = (b64.length / 4) * 3;\n    var arrayBuffer = new ArrayBuffer(ab_bytes);\n\n    b64 = removePaddingFromBase64(removePaddingFromBase64(b64));\n\n    var bytes = parseInt((b64.length / 4) * 3, 10);\n\n    var uarray;\n    var chr1, chr2, chr3;\n    var enc1, enc2, enc3, enc4;\n    var i = 0;\n    var j = 0;\n\n    uarray = new Uint8Array(arrayBuffer);\n\n    b64 = b64.replace(/[^A-Za-z0-9\\+\\/\\=]/g, \"\");\n\n    for (i = 0; i < bytes; i += 3) {\n      //get the 3 octects in 4 ascii chars\n      enc1 = _keyStr.indexOf(b64.charAt(j++));\n      enc2 = _keyStr.indexOf(b64.charAt(j++));\n      enc3 = _keyStr.indexOf(b64.charAt(j++));\n      enc4 = _keyStr.indexOf(b64.charAt(j++));\n\n      chr1 = (enc1 << 2) | (enc2 >> 4);\n      chr2 = ((enc2 & 15) << 4) | (enc3 >> 2);\n      chr3 = ((enc3 & 3) << 6) | enc4;\n\n      uarray[i] = chr1;\n      if (enc3 !== 64) {\n        uarray[i + 1] = chr2;\n      }\n      if (enc4 !== 64) {\n        uarray[i + 2] = chr3;\n      }\n    }\n\n    // https://webaudio.github.io/web-audio-api/#dom-baseaudiocontext-decodeaudiodata\n    // Asynchronously decodes the audio file data contained in the ArrayBuffer.\n    audioContext.decodeAudioData(\n      arrayBuffer, // has its content-type determined by sniffing\n      function (buffer) { // successCallback, argument is an AudioBuffer representing the decoded PCM audio data.\n        // source.buffer = buffer;\n        // source.loop = true;\n        // source.start(0);\n        let float32ArrayBuffer = buffer.getChannelData(0);\n        if (data !== undefined && audioWorkletNode !== undefined) {\n          // console.log('f32array: ' + float32Array);\n          audioWorkletNode.port.postMessage({\n            \"sample\":sampleObjectName,\n            \"buffer\": float32ArrayBuffer,\n          });\n        }\n      },\n      function (buffer) { // errorCallback\n        console.log(\"Error decoding source!\");\n      }\n    );\n  } else {\n    // Load asynchronously\n    // NOTE: This is giving me an error\n    // Uncaught ReferenceError: XMLHttpRequest is not defined (index):97 MaxiProcessor Error detected: undefined\n    // NOTE: followed the trail to the wasmmodule.js\n    // when loading on if (typeof XMLHttpRequest !== 'undefined') {\n    // throw new Error(\"Lazy loading should have been performed (contents set) in createLazyFile, but it was not. Lazy loading only works in web workers.\n    // Use --embed-file or --preload-file in emcc on the main thread.\");\n    var request = new XMLHttpRequest();\n    request.addEventListener(\"load\", () => console.log(\"The transfer is complete.\"));\n    request.open(\"GET\", url, true);\n    request.responseType = \"arraybuffer\";\n    request.onload = function () {\n      audioContext.decodeAudioData(\n        request.response,\n        function (buffer) {\n          let float32ArrayBuffer = buffer.getChannelData(0);\n          if (data !== undefined && audioWorkletNode !== undefined) {\n            // console.log('f32array: ' + float32Array);\n            audioWorkletNode.port.postMessage({\n              \"sample\":sampleObjectName,\n              \"buffer\": float32ArrayBuffer,\n            });\n          }\n        },\n        function (buffer) {\n          console.log(\"Error decoding source!\");\n        }\n      );\n    };\n    request.send();\n  }\n  return \"Loading module\";\n};\n\n/**\n * @buildWorkletStringForBlob\n */\nconst buildWorkletStringForBlob = () => {\n  let userDefinedFunction = \"\";\n  switch (expression % 2) {\n    case 0:\n      userDefinedFunction = `Math.random() * 2`;\n      break;\n    case 1:\n      userDefinedFunction = `(Math.sin(400) + 0.4)`;\n      break;\n    default:\n      userDefinedFunction = `(Math.sin(440) + 0.4)`;\n  }\n\n  // We get an \"Error on loading worklet:  DOMException\" with the following import:\n  // import Module from './maximilian.wasmmodule.js';\n  return `\n      import Module from './maximilian.wasmmodule.js';\n      cwlass CustomProcessor extends AudioWorkletProcessor {\n        static get parameterDescriptors() {\n          return [{\n            name: 'gain',\n            defaultValue: 0.1\n          }];\n        }\n        constructor() {\n          super();\n          this.sampleRate = 44100;\n\n          this.port.onmessage = (event) => {\n            console.log(event.data);\n          };\n\n        }\n        process(inputs, outputs, parameters) {\n\n          const outputsLength = outputs.length;\n          for (let outputId = 0; outputId < outputsLength; ++outputId) {\n            let output = outputs[outputId];\n            const channelLenght = output.length;\n\n            for (let channelId = 0; channelId < channelLenght; ++channelId) {\n              const gain = parameters.gain;\n              const isConstant = gain.length === 1\n              let outputChannel = output[channelId];\n\n              for (let i = 0; i < outputChannel.length; ++i) {\n                const amp = isConstant ? gain[0] : gain[i]\n                outputChannel[i] = ${userDefinedFunction} * amp;\n              }\n            }\n          }\n          return true;\n        }\n      }`;\n}\n\n/**\n * @createAndRegisterCustomProcessorCode\n */\nconst createAndRegisterCustomProcessorCode = (il2pCode, processorName) => {\n\n  return `${il2pCode}\n\n    registerProcessor(\"${processorName}\", CustomProcessor);`;\n}\n\n/**\n * @buildWorkletStringForBlob\n */\nconst buildWorkletFromBlob = () => {\n  console.log('processorCount: ' + undefined.processorCount);\n  // const userCode = editor.getDoc().getValue();\n  const processorName = `processor-${undefined.processorCount}`;\n\n  undefined.il2pCode = undefined.translateIntermediateLanguageToProcessorCode(undefined.processorCount);\n\n  const code = undefined.createAndRegisterCustomProcessorCode(undefined.il2pCode, processorName);\n\n  console.log(code);\n\n  const blob = new Blob([code], {\n    type: \"application/javascript; charset=utf-8\",\n  });\n\n  return blob;\n}\n\n/**\n * TODO: Check for memory leaks\n * @runProcessorCode\n */\nconst runProcessorCode = () => {\n  // TODO: Check for memory leaks\n  // URL.revokeObjectURL()\n  const workletUrl = window.URL.createObjectURL(blob);\n\n  // Set custom processor in audio worklet\n  undefined.audioContext.audioWorklet.addModule(workletUrl).then(() => {\n    undefined.stop();\n    undefined.customNode = new CustomAudioNode(undefined.audioContext, processorName);\n    undefined.customNode.port.onmessage = (event) => {\n      //  data from the processor.\n      console.log(\"from processor: \" + event.data);\n    };\n    undefined.customNode.connect(undefined.audioContext.destination);\n  }).catch(e => console.log(\"Error on loading worklet: \", e));\n}\n\n\nconst generateNoiseBuffer = (length) => {\n  var bufferData = new Module.VectorDouble();\n  for (var n = 0; n < length; n++) {\n    bufferData.push_back(Math.random(1));\n  }\n  return bufferData;\n}\n\n\nconst translateBlobToBuffer = (blob) => {\n\n  let arrayBuffer = null;\n  let float32Array = null;\n  var fileReader = new FileReader();\n  fileReader.onload = function (event) {\n    arrayBuffer = event.target.result;\n    float32Array = new Float32Array(arrayBuffer);\n  };\n  fileReader.readAsArrayBuffer(blob);\n  let audioFloat32Array = fileReader.result;\n  var maxiSampleBufferData = new Module.VectorDouble();\n  for (var i = 0; i < audioFloat32Array.length; i++) {\n    maxiSampleBufferData.push_back(audioFloat32Array[i]);\n  }\n  return maxiSampleBufferData;\n}\n\n\n//# sourceURL=webpack://sema-engine/./src/engine/maximilian.util.js?");

/***/ }),

/***/ "./src/index.js":
/*!**********************!*\
  !*** ./src/index.js ***!
  \**********************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"AudioEngine\": () => /* reexport safe */ _engine_audioEngine_js__WEBPACK_IMPORTED_MODULE_0__.AudioEngine\n/* harmony export */ });\n/* harmony import */ var _engine_audioEngine_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./engine/audioEngine.js */ \"./src/engine/audioEngine.js\");\n\n// import Maximilian from 'maximilian-wasm/maximilian.wasm';\n\n// console.log(Maximilian);\n// This fails because it is not importing Nearley node module\n// export { compile } from '../src/compiler/compiler.js';\n\n//# sourceURL=webpack://sema-engine/./src/index.js?");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		if(__webpack_module_cache__[moduleId]) {
/******/ 			return __webpack_module_cache__[moduleId].exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/global */
/******/ 	(() => {
/******/ 		__webpack_require__.g = (function() {
/******/ 			if (typeof globalThis === 'object') return globalThis;
/******/ 			try {
/******/ 				return this || new Function('return this')();
/******/ 			} catch (e) {
/******/ 				if (typeof window === 'object') return window;
/******/ 			}
/******/ 		})();
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => Object.prototype.hasOwnProperty.call(obj, prop)
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/publicPath */
/******/ 	(() => {
/******/ 		var scriptUrl;
/******/ 		if (__webpack_require__.g.importScripts) scriptUrl = __webpack_require__.g.location + "";
/******/ 		var document = __webpack_require__.g.document;
/******/ 		if (!scriptUrl && document) {
/******/ 			if (document.currentScript)
/******/ 				scriptUrl = document.currentScript.src
/******/ 			if (!scriptUrl) {
/******/ 				var scripts = document.getElementsByTagName("script");
/******/ 				if(scripts.length) scriptUrl = scripts[scripts.length - 1].src
/******/ 			}
/******/ 		}
/******/ 		// When supporting browsers where an automatic publicPath is not supported you must specify an output.publicPath manually via configuration
/******/ 		// or pass an empty string ("") and set the __webpack_public_path__ variable from your code to use your own logic.
/******/ 		if (!scriptUrl) throw new Error("Automatic publicPath is not supported in this browser");
/******/ 		scriptUrl = scriptUrl.replace(/#.*$/, "").replace(/\?.*$/, "").replace(/\/[^\/]+$/, "/");
/******/ 		__webpack_require__.p = scriptUrl;
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	// module exports must be returned from runtime so entry inlining is disabled
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__("./src/index.js");
/******/ })()
;
});